{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed\n",
    "import math\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.layers import Layer\n",
    "import numpy as np\n",
    "import random\n",
    "import copy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Image Utilities\n",
    "def intersection_x1_y1_x2_y2(box1=None, box2=None):\n",
    "    x_i = max(box1[0], box2[0])\n",
    "    y_i = max(box1[1], box2[1])\n",
    "    width = min(box1[2], box2[2]) - x_i\n",
    "    height = min(box1[3], box2[3]) - y_i\n",
    "    return width * height\n",
    "\n",
    "\n",
    "def iou_x1_y1_x2_y2(box1=None, box2=None):\n",
    "    if box1[0] >= box2[2] or box1[2] <= box2[0] or box1[1] >= box2[3] or box1[3] <= box2[1]:\n",
    "        return 0.0\n",
    "    intersection = intersection_x1_y1_x2_y2(box1, box2)\n",
    "    # union\n",
    "    boxes_area = (box1[2]-box1[0])*(box1[3]-box1[1]) + (box2[2]-box2[0])*(box2[3]-box2[1])\n",
    "    union = boxes_area - intersection\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection/union"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "def build_vgg16_base(input_tensor=None, trainable=False, include_output_layer=False, image_shape=(400, 300)):\n",
    "\n",
    "    input_shape = (image_shape[1], image_shape[0], 3)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    bn_axis = 3\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    if include_output_layer:\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    return img_input, x\n",
    "\n",
    "\n",
    "def build_base_model(input_tensor=None, trainable=False, include_output_layer=False, image_shape=(300, 400), backbone='VGG16'):\n",
    "    img_input, shared_layers, roi_input = None, None, None\n",
    "    if backbone == 'VGG16':\n",
    "        img_input, shared_layers = build_vgg16_base(input_tensor, trainable, include_output_layer, image_shape)\n",
    "    roi_input = Input(shape=(None, 4))\n",
    "\n",
    "    return img_input, shared_layers, roi_input\n",
    "\n",
    "\n",
    "def build_rpn_layer(base_layers, num_anchors=9):\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
    "\n",
    "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
    "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
    "\n",
    "    return x_class, x_regr\n",
    "\n",
    "\n",
    "class RoiPoolingConv(Layer):\n",
    "    def __init__(self, pool_size=7, num_rois=4, **kwargs):\n",
    "        self.dim_ordering = K.image_data_format()\n",
    "        self.pool_size = pool_size\n",
    "        self.num_rois = num_rois\n",
    "\n",
    "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[0][3]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        assert (len(x) == 2)\n",
    "\n",
    "        # x[0] is image with shape (rows, cols, channels)\n",
    "        img = x[0]\n",
    "\n",
    "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
    "        rois = x[1]\n",
    "        input_shape = K.shape(img)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for roi_idx in range(self.num_rois):\n",
    "            x = rois[0, roi_idx, 0]\n",
    "            y = rois[0, roi_idx, 1]\n",
    "            w = rois[0, roi_idx, 2]\n",
    "            h = rois[0, roi_idx, 3]\n",
    "\n",
    "            x = K.cast(x, 'int32')\n",
    "            y = K.cast(y, 'int32')\n",
    "            w = K.cast(w, 'int32')\n",
    "            h = K.cast(h, 'int32')\n",
    "\n",
    "            # Resized roi of the image to pooling size (7x7)\n",
    "            rs = tf.image.resize(img[:, y:y + h, x:x + w, :], (self.pool_size, self.pool_size))\n",
    "            outputs.append(rs)\n",
    "\n",
    "        final_output = K.concatenate(outputs, axis=0)\n",
    "\n",
    "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
    "        # Might be (1, 4, 7, 7, 3)\n",
    "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
    "\n",
    "        # permute_dimensions is similar to transpose\n",
    "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'pool_size': self.pool_size,\n",
    "                  'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingConv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def classifier_layer(base_layers, input_rois, num_rois=4, nb_classes=38):\n",
    "    input_shape = (num_rois, 7, 7, 512)\n",
    "\n",
    "    pooling_regions = 7\n",
    "\n",
    "    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n",
    "    # num_rois (4) 7x7 roi pooling\n",
    "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n",
    "\n",
    "    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n",
    "    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "\n",
    "    # There are two output layer\n",
    "    # out_class: softmax acivation function for classify the class name of the object\n",
    "    # out_regr: linear activation function for bboxes coordinates regression\n",
    "    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'),\n",
    "                                name='dense_class_{}'.format(nb_classes))(out)\n",
    "    # note: no regression target for bg class\n",
    "    out_regr = TimeDistributed(Dense(4 * (nb_classes - 1), activation='linear', kernel_initializer='zero'),\n",
    "                               name='dense_regress_{}'.format(nb_classes))(out)\n",
    "\n",
    "    return [out_class, out_regr]\n",
    "\n",
    "\n",
    "def faster_rcnn(shared_layers, roi_input, img_input, num_rois=4, num_classes=38):\n",
    "    # define the RPN, built on the base layers\n",
    "    num_anchors = 3 * 3  # 9\n",
    "    rpn = list(build_rpn_layer(shared_layers, num_anchors))\n",
    "\n",
    "    classifier = classifier_layer(shared_layers, roi_input, num_rois=num_rois, nb_classes=num_classes)\n",
    "    model_rpn = Model(img_input, rpn)\n",
    "    model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "    # this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "    model_all = Model([img_input, roi_input], rpn + classifier)\n",
    "    return model_rpn, model_classifier, model_all\n",
    "\n",
    "\n",
    "def rpn_loss_regr(num_anchors, lambda_rpn_regr=1.5):\n",
    "    def rpn_loss_regr_fixed_num(y_true, y_pred):\n",
    "\n",
    "        # x is the difference between true value and predicted vaue\n",
    "        x = y_true[:, :, :, 4 * num_anchors:] - y_pred\n",
    "\n",
    "        # absolute value of x\n",
    "        x_abs = K.abs(x)\n",
    "\n",
    "        # If x_abs <= 1.0, x_bool = 1\n",
    "        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n",
    "\n",
    "        return lambda_rpn_regr * K.sum(\n",
    "            y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])\n",
    "\n",
    "    return rpn_loss_regr_fixed_num\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loss\n",
    "\n",
    "def rpn_loss_cls(num_anchors, lambda_rpn_class=1.0):\n",
    "    def rpn_loss_cls_fixed_num(y_true, y_pred):\n",
    "\n",
    "            return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n",
    "\n",
    "    return rpn_loss_cls_fixed_num\n",
    "\n",
    "\n",
    "def class_loss_regr(num_classes, lambda_cls_regr=1.0):\n",
    "    def class_loss_regr_fixed_num(y_true, y_pred):\n",
    "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
    "        x_abs = K.abs(x)\n",
    "        x_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n",
    "        return lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n",
    "    return class_loss_regr_fixed_num\n",
    "\n",
    "epsilon = 1e-4\n",
    "\n",
    "def class_loss_cls(y_true, y_pred, lambda_cls_class=1.0):\n",
    "    return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))\n",
    "\n",
    "\n",
    "\n",
    "def calc_iou(R, annotation, classes_count=38, rpn_stride=16, classifier_min_overlap=0.1, classifier_max_overlap=0.3,\n",
    "             classifier_regr_std=[8.0, 8.0, 4.0, 4.0]):\n",
    "\n",
    "    gta = np.zeros((len(annotation), 4))\n",
    "\n",
    "    for bbox_num, bbox in enumerate(annotation):\n",
    "        # get the GT box coordinates, and resize to account for image resizing\n",
    "        # gta[bbox_num, 0] = (40 * (600 / 800)) / 16 = int(round(1.875)) = 2 (x in feature map)\n",
    "        bbox = bbox[1:]\n",
    "        gta[bbox_num, 0] = int(round((bbox[0] - bbox[2]/2) / rpn_stride))\n",
    "        gta[bbox_num, 1] = int(round((bbox[0] + bbox[2]/2) / rpn_stride))\n",
    "        gta[bbox_num, 2] = int(round((bbox[1] - bbox[3]/2) / rpn_stride))\n",
    "        gta[bbox_num, 3] = int(round((bbox[1] + bbox[3]/2) / rpn_stride))\n",
    "\n",
    "        # gta[bbox_num, 0] = int(bbox[0] / rpn_stride)\n",
    "        # gta[bbox_num, 1] = int(bbox[1] / rpn_stride)\n",
    "        # gta[bbox_num, 2] = int(bbox[2] / rpn_stride)\n",
    "        # gta[bbox_num, 3] = int(bbox[3] / rpn_stride)\n",
    "\n",
    "\n",
    "    x_roi = []\n",
    "    y_class_num = []\n",
    "    y_class_regr_coords = []\n",
    "    y_class_regr_label = []\n",
    "    IoUs = []  # for debugging only\n",
    "\n",
    "    # R.shape[0]: number of bboxes (=300 from non_max_suppression)\n",
    "    for ix in range(R.shape[0]):\n",
    "        (x1, y1, x2, y2) = R[ix, :]\n",
    "        x1 = int(round(x1))\n",
    "        y1 = int(round(y1))\n",
    "        x2 = int(round(x2))\n",
    "        y2 = int(round(y2))\n",
    "\n",
    "        best_iou = 0.0\n",
    "        best_bbox = -1\n",
    "        # Iterate through all the ground-truth bboxes to calculate the iou\n",
    "        for bbox_num in range(len(annotation)):\n",
    "            curr_iou = iou_x1_y1_x2_y2(\n",
    "                [gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]],\n",
    "                [x1, y1, x2, y2])\n",
    "\n",
    "            # Find out the corresponding ground-truth bbox_num with larget iou\n",
    "            if curr_iou > best_iou:\n",
    "                best_iou = curr_iou\n",
    "                best_bbox = bbox_num\n",
    "\n",
    "        if best_iou < classifier_min_overlap:\n",
    "            continue\n",
    "        else:\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            x_roi.append([x1, y1, w, h])\n",
    "            IoUs.append(best_iou)\n",
    "\n",
    "            if classifier_min_overlap <= best_iou < classifier_max_overlap:\n",
    "                # hard negative example\n",
    "                cls_name = classes_count - 1\n",
    "            elif classifier_max_overlap <= best_iou:\n",
    "                cls_name = annotation[best_bbox][0] - 1\n",
    "                # cls_name = annotation[best_bbox][0]\n",
    "                cxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n",
    "                cyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n",
    "\n",
    "                cx = x1 + w / 2.0\n",
    "                cy = y1 + h / 2.0\n",
    "\n",
    "                tx = (cxg - cx) / float(w)\n",
    "                ty = (cyg - cy) / float(h)\n",
    "                tw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n",
    "                th = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n",
    "            else:\n",
    "                print('roi = {}'.format(best_iou))\n",
    "                raise RuntimeError\n",
    "\n",
    "        class_num = cls_name\n",
    "        class_label = classes_count * [0]\n",
    "        class_label[class_num] = 1\n",
    "        y_class_num.append(copy.deepcopy(class_label))\n",
    "        coords = [0] * 4 * (classes_count - 1)\n",
    "        labels = [0] * 4 * (classes_count - 1)\n",
    "        if cls_name != classes_count - 1:\n",
    "            label_pos = 4 * class_num\n",
    "            sx, sy, sw, sh = classifier_regr_std\n",
    "            coords[label_pos:4 + label_pos] = [sx * tx, sy * ty, sw * tw, sh * th]\n",
    "            labels[label_pos:4 + label_pos] = [1, 1, 1, 1]\n",
    "            y_class_regr_coords.append(copy.deepcopy(coords))\n",
    "            y_class_regr_label.append(copy.deepcopy(labels))\n",
    "        else:\n",
    "            y_class_regr_coords.append(copy.deepcopy(coords))\n",
    "            y_class_regr_label.append(copy.deepcopy(labels))\n",
    "\n",
    "    if len(x_roi) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n",
    "    X = np.array(x_roi, dtype='float32')\n",
    "    # one hot code for bboxes from above => x_roi (X)\n",
    "    Y1 = np.array(y_class_num, dtype='float32')\n",
    "    # corresponding labels and corresponding gt bboxes\n",
    "    Y2 = np.concatenate([np.array(y_class_regr_label, dtype='float32'),\n",
    "                         np.array(y_class_regr_coords, dtype='float32')], axis=1)\n",
    "\n",
    "    return np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_rpn(annotation, image_width=400, image_height=300,\n",
    "             anchor_box_scales=[64, 128, 256], anchor_box_ratios=[1, 2, 1/2], rpn_max_overlap=0.7, rpn_min_overlap=0.3):\n",
    "    downscale = 16.0\n",
    "    anchor_sizes = anchor_box_scales\n",
    "    anchor_ratios = anchor_box_ratios\n",
    "    num_anchors = len(anchor_sizes) * len(anchor_ratios)  # 3x3=9\n",
    "\n",
    "    # calculate the output map size based on the network architecture\n",
    "    (num_anchor_cx, num_anchor_cy) = (image_width // 16, image_height // 16)\n",
    "    n_anchratios = len(anchor_ratios)  # 3\n",
    "\n",
    "    # initialise empty output objectives\n",
    "    y_rpn_overlap = np.zeros((num_anchor_cy, num_anchor_cx, num_anchors))\n",
    "    y_is_box_valid = np.zeros((num_anchor_cy, num_anchor_cx, num_anchors))\n",
    "    y_rpn_regr = np.zeros((num_anchor_cy, num_anchor_cx, num_anchors * 4))\n",
    "\n",
    "    num_bboxes = len(annotation)\n",
    "\n",
    "    num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n",
    "    best_anchor_for_bbox = -1 * np.ones((num_bboxes, 4)).astype(int)\n",
    "    best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n",
    "    best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n",
    "    best_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n",
    "\n",
    "    # get the GT box coordinates, and resize to account for image resizing\n",
    "    gta = np.zeros((num_bboxes, 4))\n",
    "    for bbox_num, bbox in enumerate(annotation):\n",
    "        # get the GT box coordinates, and resize to account for image resizing\n",
    "        gta[bbox_num, 0] = bbox[0] - bbox[2]/2\n",
    "        gta[bbox_num, 1] = bbox[0] + bbox[2]/2\n",
    "        gta[bbox_num, 2] = bbox[1] - bbox[3]/2\n",
    "        gta[bbox_num, 3] = bbox[1] + bbox[3]/2\n",
    "\n",
    "    # rpn ground truth\n",
    "    for anchor_size_idx, anchor_size in enumerate(anchor_sizes):\n",
    "        for anchor_ratio_idx, anchor_ratio in enumerate(anchor_ratios):\n",
    "            anchor_x = anchor_size * anchor_ratio\n",
    "            anchor_y = anchor_size / anchor_ratio\n",
    "            candidate_idx = anchor_ratio_idx + n_anchratios * anchor_size_idx\n",
    "            for ix in range(num_anchor_cx):\n",
    "                # x-coordinates of the current anchor box\n",
    "                x1_anc = downscale * (ix + 0.5) - anchor_x / 2\n",
    "                x2_anc = downscale * (ix + 0.5) + anchor_x / 2\n",
    "\n",
    "                # ignore boxes that go across image boundaries\n",
    "                if x1_anc < 0 or x2_anc > image_width:\n",
    "                    continue\n",
    "\n",
    "                for jy in range(num_anchor_cy):\n",
    "\n",
    "                    # y-coordinates of the current anchor box\n",
    "                    y1_anc = downscale * (jy + 0.5) - anchor_y / 2\n",
    "                    y2_anc = downscale * (jy + 0.5) + anchor_y / 2\n",
    "\n",
    "                    # ignore boxes that go across image boundaries\n",
    "                    if y1_anc < 0 or y2_anc > image_height:\n",
    "                        continue\n",
    "\n",
    "                    # bbox_type indicates whether an anchor should be a target\n",
    "                    # Initialize with 'negative'\n",
    "                    bbox_type = 'neg'\n",
    "\n",
    "                    # this is the best IOU for the (x,y) coord and the current anchor\n",
    "                    # note that this is different from the best IOU for a GT bbox\n",
    "                    best_iou_for_loc = 0.0\n",
    "\n",
    "                    cxa = (x1_anc + x2_anc) / 2.0\n",
    "                    cya = (y1_anc + y2_anc) / 2.0\n",
    "                    tx, ty, tw, th = None, None, None, None\n",
    "                    best_regr = None\n",
    "                    for bbox_num in range(num_bboxes):\n",
    "\n",
    "                        # get IOU of the current GT box and the current anchor box\n",
    "                        curr_iou = iou_x1_y1_x2_y2([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]],\n",
    "                                       [x1_anc, y1_anc, x2_anc, y2_anc])\n",
    "                        # calculate the regression targets if they will be needed\n",
    "                        if curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > rpn_max_overlap:\n",
    "                            cx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n",
    "                            cy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n",
    "\n",
    "                            tx = (cx - cxa) / (x2_anc - x1_anc)\n",
    "                            ty = (cy - cya) / (y2_anc - y1_anc)\n",
    "                            tw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n",
    "                            th = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n",
    "\n",
    "                        # all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n",
    "                        if curr_iou > best_iou_for_bbox[bbox_num]:\n",
    "                            best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n",
    "                            best_iou_for_bbox[bbox_num] = curr_iou\n",
    "                            best_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]\n",
    "                            best_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]\n",
    "\n",
    "                        # we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)\n",
    "                        if curr_iou > rpn_max_overlap:\n",
    "                            bbox_type = 'pos'\n",
    "                            num_anchors_for_bbox[bbox_num] += 1\n",
    "                            # we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n",
    "                            if curr_iou > best_iou_for_loc:\n",
    "                                best_iou_for_loc = curr_iou\n",
    "                                best_regr = (tx, ty, tw, th)\n",
    "\n",
    "                        # if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective\n",
    "                        if rpn_min_overlap < curr_iou < rpn_max_overlap:\n",
    "                            # gray zone between neg and pos\n",
    "                            if bbox_type != 'pos':\n",
    "                                bbox_type = 'neutral'\n",
    "\n",
    "                    # turn on or off outputs depending on IOUs\n",
    "                    if bbox_type == 'neg':\n",
    "                        y_is_box_valid[jy, ix, candidate_idx] = 1\n",
    "                        y_rpn_overlap[jy, ix, candidate_idx] = 0\n",
    "                    elif bbox_type == 'neutral':\n",
    "                        y_is_box_valid[jy, ix, candidate_idx] = 0\n",
    "                        y_rpn_overlap[jy, ix, candidate_idx] = 0\n",
    "                    elif bbox_type == 'pos':\n",
    "                        y_is_box_valid[jy, ix, candidate_idx] = 1\n",
    "                        y_rpn_overlap[jy, ix, candidate_idx] = 1\n",
    "                        start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n",
    "                        y_rpn_regr[jy, ix, start:start + 4] = best_regr\n",
    "\n",
    "    # we ensure that every bbox has at least one positive RPN region\n",
    "\n",
    "    for idx in range(num_anchors_for_bbox.shape[0]):\n",
    "        if num_anchors_for_bbox[idx] == 0:\n",
    "            # no box with an IOU greater than zero ...\n",
    "            if best_anchor_for_bbox[idx, 0] == -1:\n",
    "                continue\n",
    "            y_is_box_valid[\n",
    "                best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], best_anchor_for_bbox[\n",
    "                    idx, 2] + n_anchratios *\n",
    "                best_anchor_for_bbox[idx, 3]] = 1\n",
    "            y_rpn_overlap[\n",
    "                best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], best_anchor_for_bbox[\n",
    "                    idx, 2] + n_anchratios *\n",
    "                best_anchor_for_bbox[idx, 3]] = 1\n",
    "            start = 4 * (best_anchor_for_bbox[idx, 2] + n_anchratios * best_anchor_for_bbox[idx, 3])\n",
    "            y_rpn_regr[\n",
    "            best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], start:start + 4] = best_dx_for_bbox[idx, :]\n",
    "\n",
    "    y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n",
    "    y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n",
    "\n",
    "    y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n",
    "    y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n",
    "\n",
    "    y_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n",
    "    y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n",
    "\n",
    "    pos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n",
    "    neg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n",
    "\n",
    "    num_pos = len(pos_locs[0])\n",
    "\n",
    "    # one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative\n",
    "    # regions. We also limit it to 256 regions.\n",
    "    num_regions = 256\n",
    "\n",
    "    if len(pos_locs[0]) > num_regions / 2:\n",
    "        val_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions / 2)\n",
    "        y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n",
    "        num_pos = num_regions / 2\n",
    "\n",
    "    if len(neg_locs[0]) + num_pos > num_regions:\n",
    "        val_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n",
    "        y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n",
    "\n",
    "    y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n",
    "    y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n",
    "\n",
    "    return np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_anchor_gt(all_img_data, mode='train', img_channel_mean=[103.939, 116.779, 123.68], std_scaling=4.0):\n",
    "    for (x_img, img_data) in all_img_data:\n",
    "        (rows, cols, _) = x_img.shape\n",
    "\n",
    "        y_rpn_cls, y_rpn_regr, num_pos = calc_rpn(img_data)\n",
    "        x_img = x_img[:, :, (2, 1, 0)]  # BGR -> RGB\n",
    "        x_img = x_img.astype(np.float32)\n",
    "        # x_img[:, :, 0] -= img_channel_mean[0]\n",
    "        # x_img[:, :, 1] -= img_channel_mean[1]\n",
    "        # x_img[:, :, 2] -= img_channel_mean[2]\n",
    "        x_img /= 255.0\n",
    "\n",
    "        x_img = np.transpose(x_img, (2, 0, 1))\n",
    "        x_img = np.expand_dims(x_img, axis=0)\n",
    "\n",
    "        y_rpn_regr[:, y_rpn_regr.shape[1] // 2:, :, :] *= std_scaling\n",
    "\n",
    "        x_img = np.transpose(x_img, (0, 2, 3, 1))\n",
    "        y_rpn_cls = np.transpose(y_rpn_cls, (0, 3, 2, 1))\n",
    "        y_rpn_regr = np.transpose(y_rpn_regr, (0, 3, 2, 1))\n",
    "\n",
    "        return np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_regr_np(X, T):\n",
    "    try:\n",
    "        x = X[0, :, :]\n",
    "        y = X[1, :, :]\n",
    "        w = X[2, :, :]\n",
    "        h = X[3, :, :]\n",
    "\n",
    "        tx = T[0, :, :]\n",
    "        ty = T[1, :, :]\n",
    "        tw = T[2, :, :]\n",
    "        th = T[3, :, :]\n",
    "\n",
    "        cx = x + w / 2.\n",
    "        cy = y + h / 2.\n",
    "        cx1 = tx * w + cx\n",
    "        cy1 = ty * h + cy\n",
    "\n",
    "        w1 = np.exp(tw.astype(np.float64)) * w\n",
    "        h1 = np.exp(th.astype(np.float64)) * h\n",
    "        x1 = cx1 - w1 / 2.\n",
    "        y1 = cy1 - h1 / 2.\n",
    "\n",
    "        x1 = np.round(x1)\n",
    "        y1 = np.round(y1)\n",
    "        w1 = np.round(w1)\n",
    "        h1 = np.round(h1)\n",
    "        return np.stack([x1, y1, w1, h1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_regr(x, y, w, h, tx, ty, tw, th):\n",
    "    # Apply regression to x, y, w and h\n",
    "    try:\n",
    "        cx = x + w / 2.\n",
    "        cy = y + h / 2.\n",
    "        cx1 = tx * w + cx\n",
    "        cy1 = ty * h + cy\n",
    "        w1 = math.exp(tw) * w\n",
    "        h1 = math.exp(th) * h\n",
    "        x1 = cx1 - w1 / 2.\n",
    "        y1 = cy1 - h1 / 2.\n",
    "        x1 = int(round(x1))\n",
    "        y1 = int(round(y1))\n",
    "        w1 = int(round(w1))\n",
    "        h1 = int(round(h1))\n",
    "\n",
    "        return x1, y1, w1, h1\n",
    "\n",
    "    except ValueError:\n",
    "        return x, y, w, h\n",
    "    except OverflowError:\n",
    "        return x, y, w, h\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return x, y, w, h\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n",
    "    # code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "    # if there are no boxes, return an empty list\n",
    "\n",
    "    # Process explanation:\n",
    "    #   Step 1: Sort the probs list\n",
    "    #   Step 2: Find the larget prob 'Last' in the list and save it to the pick list\n",
    "    #   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list\n",
    "    #   Step 4: Repeat step 2 and step 3 until there is no item in the probs list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    np.testing.assert_array_less(x1, x2)\n",
    "    np.testing.assert_array_less(y1, y2)\n",
    "\n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "\n",
    "    # calculate the areas\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # sort the bounding boxes\n",
    "    idxs = np.argsort(probs)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the intersection\n",
    "\n",
    "        xx1_int = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1_int = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2_int = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2_int = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        ww_int = np.maximum(0, xx2_int - xx1_int)\n",
    "        hh_int = np.maximum(0, yy2_int - yy1_int)\n",
    "\n",
    "        area_int = ww_int * hh_int\n",
    "\n",
    "        # find the union\n",
    "        area_union = area[i] + area[idxs[:last]] - area_int\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = area_int / (area_union + 1e-6)\n",
    "\n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "                                               np.where(overlap > overlap_thresh)[0])))\n",
    "\n",
    "        if len(pick) >= max_boxes:\n",
    "            break\n",
    "\n",
    "    # return only the bounding boxes that were picked using the integer data type\n",
    "    boxes = boxes[pick].astype(\"int\")\n",
    "    probs = probs[pick]\n",
    "    return boxes, probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rpn_to_roi(rpn_layer, regr_layer, use_regr=True, max_boxes=300, overlap_thresh=0.9, std_scaling=4.0,\n",
    "               anchor_box_scales=[64, 128, 256], anchor_box_ratios=[1, 2, 1/2], rpn_stride=16):\n",
    "    regr_layer = regr_layer / std_scaling\n",
    "\n",
    "    anchor_sizes = anchor_box_scales  # (3 in here)\n",
    "    anchor_ratios = anchor_box_ratios  # (3 in here)\n",
    "\n",
    "    (rows, cols) = rpn_layer.shape[1:3]\n",
    "\n",
    "    curr_layer = 0\n",
    "\n",
    "    # A.shape = (4, feature_map.height, feature_map.width, num_anchors)\n",
    "    # Might be (4, 18, 25, 18) if resized image is 400 width and 300\n",
    "    # A is the coordinates for 9 anchors for every point in the feature map\n",
    "    # => all 18x25x9=4050 anchors cooridnates\n",
    "    A = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n",
    "\n",
    "    for anchor_size in anchor_sizes:\n",
    "        for anchor_ratio in anchor_ratios:\n",
    "            # anchor_x = (128 * 1) / 16 = 8  => width of current anchor\n",
    "            # anchor_y = (128 * 2) / 16 = 16 => height of current anchor\n",
    "            anchor_x = (anchor_size * anchor_ratio) / rpn_stride\n",
    "            anchor_y = (anchor_size / anchor_ratio) / rpn_stride\n",
    "\n",
    "            # curr_layer: 0~8 (9 anchors)\n",
    "            # the Kth anchor of all position in the feature map (9th in total)\n",
    "            regr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]  # shape => (18, 25, 4)\n",
    "            regr = np.transpose(regr, (2, 0, 1))  # shape => (4, 18, 25)\n",
    "\n",
    "            # Create 18x25 mesh grid\n",
    "            # For every point in x, there are all the y points and vice versa\n",
    "            # X.shape = (18, 25)\n",
    "            # Y.shape = (18, 25)\n",
    "            X, Y = np.meshgrid(np.arange(cols), np.arange(rows))\n",
    "\n",
    "            # Calculate anchor position and size for each feature map point\n",
    "            A[0, :, :, curr_layer] = X - anchor_x / 2  # Top left x coordinate\n",
    "            A[1, :, :, curr_layer] = Y - anchor_y / 2  # Top left y coordinate\n",
    "            A[2, :, :, curr_layer] = anchor_x  # width of current anchor\n",
    "            A[3, :, :, curr_layer] = anchor_y  # height of current anchor\n",
    "\n",
    "            # Apply regression to x, y, w and h if there is rpn regression layer\n",
    "            if use_regr:\n",
    "                A[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n",
    "\n",
    "            # Avoid width and height exceeding 1\n",
    "            A[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n",
    "            A[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n",
    "\n",
    "            # Convert (x, y , w, h) to (x1, y1, x2, y2)\n",
    "            # x1, y1 is top left coordinate\n",
    "            # x2, y2 is bottom right coordinate\n",
    "            A[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n",
    "            A[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n",
    "\n",
    "            # Avoid bboxes drawn outside the feature map\n",
    "            A[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n",
    "            A[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n",
    "            A[2, :, :, curr_layer] = np.minimum(cols - 1, A[2, :, :, curr_layer])\n",
    "            A[3, :, :, curr_layer] = np.minimum(rows - 1, A[3, :, :, curr_layer])\n",
    "\n",
    "            curr_layer += 1\n",
    "\n",
    "    all_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n",
    "    all_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))  # shape=(4050,)\n",
    "\n",
    "    x1 = all_boxes[:, 0]\n",
    "    y1 = all_boxes[:, 1]\n",
    "    x2 = all_boxes[:, 2]\n",
    "    y2 = all_boxes[:, 3]\n",
    "\n",
    "    # Find out the bboxes which is illegal and delete them from bboxes list\n",
    "    idxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n",
    "\n",
    "    all_boxes = np.delete(all_boxes, idxs, 0)\n",
    "    all_probs = np.delete(all_probs, idxs, 0)\n",
    "\n",
    "    # Apply non_max_suppression\n",
    "    # Only extract the bboxes. Don't need rpn probs in the later process\n",
    "    result = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  Build faster rcnn model\n",
    "img_input, shared_layers, roi_input = build_base_model()\n",
    "rpn_class, rpn_position = build_rpn_layer(shared_layers)\n",
    "rpn_class_model = Model(img_input, rpn_class)\n",
    "rpn_positoin_model = Model(img_input, rpn_position)\n",
    "vgg_model = Model(img_input, shared_layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the models\n",
    "model_rpn, model_classifier, model_all = faster_rcnn(shared_layers, roi_input, img_input)\n",
    "model_rpn.compile(optimizer='adam', loss=[rpn_loss_cls(9), rpn_loss_regr(9)])\n",
    "model_classifier.compile(optimizer='adam',\n",
    "                         loss=[class_loss_cls, class_loss_regr(37)],\n",
    "                         metrics={'dense_class_{}'.format(38): 'accuracy'})\n",
    "model_all.compile(optimizer='sgd', loss='mae')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hyperparameters and settings\n",
    "epoch_length = 1\n",
    "num_epochs = 200\n",
    "iter_num = 0\n",
    "losses = np.zeros((epoch_length, 5))\n",
    "num_rois = 4\n",
    "best_loss = float('inf')\n",
    "output_model_name = \"weight/trash_model\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "input_img_data = np.load('data/training_input.npy', allow_pickle=True)\n",
    "label_data = np.load('data/training_output.npy', allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch_num in range(num_epochs):\n",
    "    rpn_accuracy_for_epoch = []\n",
    "    for img, annotation in zip(input_img_data, label_data):\n",
    "        # Generate X (x_img) and label Y ([y_rpn_cls, y_rpn_regr])\n",
    "        X, Y = get_anchor_gt([(img, annotation)])\n",
    "\n",
    "        loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "        P_rpn = model_rpn.predict_on_batch(X)\n",
    "        # R: bboxes (shape=(300,4))\n",
    "        # Convert rpn layer to roi bboxes\n",
    "        R = rpn_to_roi(P_rpn[0], P_rpn[1], use_regr=True, overlap_thresh=0.9, max_boxes=300)\n",
    "        X2, Y1, Y2, IouS = calc_iou(R, annotation, classes_count=38)\n",
    "\n",
    "        # If X2 is None means there are no matching bboxes\n",
    "        if X2 is None:\n",
    "            rpn_accuracy_for_epoch.append(0)\n",
    "            continue\n",
    "\n",
    "        # Find out the positive anchors and negative anchors\n",
    "        neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "        pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "        if len(neg_samples) > 0:\n",
    "            neg_samples = neg_samples[0]\n",
    "        else:\n",
    "            neg_samples = []\n",
    "\n",
    "        if len(pos_samples) > 0:\n",
    "            pos_samples = pos_samples[0]\n",
    "        else:\n",
    "            pos_samples = []\n",
    "\n",
    "        rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "\n",
    "        if num_rois > 1:\n",
    "            # If number of positive anchors is larger than 4//2 = 2, randomly choose 2 pos samples\n",
    "            if len(pos_samples) < num_rois // 2:\n",
    "                selected_pos_samples = pos_samples.tolist()\n",
    "            else:\n",
    "                selected_pos_samples = np.random.choice(pos_samples, num_rois // 2, replace=False).tolist()\n",
    "\n",
    "            # Randomly choose (num_rois - num_pos) neg samples\n",
    "            try:\n",
    "                selected_neg_samples = np.random.choice(neg_samples, num_rois - len(selected_pos_samples),\n",
    "                                                        replace=False).tolist()\n",
    "            except:\n",
    "                selected_neg_samples = np.random.choice(neg_samples, num_rois - len(selected_pos_samples),\n",
    "                                                        replace=True).tolist()\n",
    "\n",
    "            # Save all the pos and neg samples in sel_samples\n",
    "            sel_samples = selected_pos_samples + selected_neg_samples\n",
    "        else:\n",
    "            # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "            selected_pos_samples = pos_samples.tolist()\n",
    "            selected_neg_samples = neg_samples.tolist()\n",
    "            if np.random.randint(0, 2):\n",
    "                sel_samples = random.choice(neg_samples)\n",
    "            else:\n",
    "                sel_samples = random.choice(pos_samples)\n",
    "\n",
    "        loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]],\n",
    "                                                     [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "\n",
    "        losses[iter_num, 0] = loss_rpn[1]\n",
    "        losses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "        losses[iter_num, 2] = loss_class[1]\n",
    "        losses[iter_num, 3] = loss_class[2]\n",
    "        losses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "    iter_num += 1\n",
    "    print(\"Iter_num: {}\".format(iter_num))\n",
    "\n",
    "    if iter_num == epoch_length:\n",
    "        loss_rpn_cls = np.mean(losses[:, 0])\n",
    "        loss_rpn_regr = np.mean(losses[:, 1])\n",
    "        loss_class_cls = np.mean(losses[:, 2])\n",
    "        loss_class_regr = np.mean(losses[:, 3])\n",
    "        class_acc = np.mean(losses[:, 4])\n",
    "\n",
    "        mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "        rpn_accuracy_for_epoch = []\n",
    "\n",
    "        curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "        iter_num = 0\n",
    "\n",
    "        if curr_loss < best_loss:\n",
    "            print('Total loss decreased from {} to {}, saving weights'.format(best_loss, curr_loss))\n",
    "            best_loss = curr_loss\n",
    "            model_all.save_weights(output_model_name)\n",
    "\n",
    "        break\n",
    "print('Training complete, exiting.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_img_channels(img):\n",
    "    \"\"\" formats the image channels based on config \"\"\"\n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def format_img(img):\n",
    "    \"\"\" formats an image for model prediction based on config \"\"\"\n",
    "    img = format_img_channels(img)\n",
    "    return img, 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "img_input, shared_layers, roi_input = build_base_model()\n",
    "rpn_class, rpn_position = build_rpn_layer(shared_layers)\n",
    "rpn_class_model = Model(img_input, rpn_class)\n",
    "rpn_positoin_model = Model(img_input, rpn_position)\n",
    "vgg_model = Model(img_input, shared_layers)\n",
    "\n",
    "model_rpn, model_classifier, model_all = faster_rcnn(shared_layers, roi_input, img_input)\n",
    "model_all.load_weights(\"./weight/trash_model\")\n",
    "model_rpn.compile(optimizer='adam', loss=[rpn_loss_cls(9), rpn_loss_regr(9)])\n",
    "model_classifier.compile(optimizer='adam',\n",
    "                         loss=[class_loss_cls, class_loss_regr(37)],\n",
    "                         metrics={'dense_class_{}'.format(38): 'accuracy'})\n",
    "model_all.compile(optimizer='sgd', loss='mae')\n",
    "\n",
    "input_img_data = np.load('data/test_input.npy', allow_pickle=True)\n",
    "visualize = False\n",
    "\n",
    "answers = []\n",
    "for idx, img in enumerate(input_img_data):\n",
    "    print(\"image number: {}\".format(idx))\n",
    "    # If the box classification value is less than this, we ignore this box\n",
    "    bbox_threshold = 0.801\n",
    "    answer_annos = []\n",
    "    X, ratio = format_img(img)\n",
    "\n",
    "    X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "    # get output layer Y1, Y2 from the RPN and the feature maps F\n",
    "    # Y1: y_rpn_cls\n",
    "    # Y2: y_rpn_regr\n",
    "    [Y1, Y2] = model_rpn.predict(X)\n",
    "\n",
    "    # Get bboxes by applying NMS\n",
    "    # R.shape = (300, 4)\n",
    "    R = rpn_to_roi(Y1, Y2, overlap_thresh=0.7)\n",
    "    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "    R[:, 2] -= R[:, 0]\n",
    "    R[:, 3] -= R[:, 1]\n",
    "\n",
    "    # apply the spatial pyramid pooling to the proposed regions\n",
    "    bboxes = {}\n",
    "    probs = {}\n",
    "\n",
    "    for jk in range(R.shape[0] // 4+ 1):\n",
    "        ROIs = np.expand_dims(R[4 * jk:4 * (jk + 1), :], axis=0)\n",
    "        if ROIs.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        if jk == R.shape[0] // 4:\n",
    "            # pad R\n",
    "            curr_shape = ROIs.shape\n",
    "            target_shape = (curr_shape[0], 4, curr_shape[2])\n",
    "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "            ROIs = ROIs_padded\n",
    "\n",
    "        [P_cls, P_regr] = model_classifier.predict([F, ROIs])\n",
    "\n",
    "        # Calculate bboxes coordinates on resized image\n",
    "        for ii in range(P_cls.shape[1]):\n",
    "            # Ignore 'bg' class\n",
    "            if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "                continue\n",
    "\n",
    "            cls_name = np.argmax(P_cls[0, ii, :])\n",
    "\n",
    "            if cls_name not in bboxes:\n",
    "                bboxes[cls_name] = []\n",
    "                probs[cls_name] = []\n",
    "\n",
    "            (x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "            cls_num = np.argmax(P_cls[0, ii, :])\n",
    "            try:\n",
    "                (tx, ty, tw, th) = P_regr[0, ii, 4 * cls_num:4 * (cls_num + 1)]\n",
    "                tx /= 8.0\n",
    "                ty /= 8.0\n",
    "                tw /= 4.0\n",
    "                th /= 4.0\n",
    "                x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "            except:\n",
    "                pass\n",
    "            bboxes[cls_name].append(\n",
    "                [16 * x, 16 * y, 16 * (x + w), 16 * (y + h)])\n",
    "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "    all_dets = []\n",
    "\n",
    "    for key in bboxes:\n",
    "        bbox = np.array(bboxes[key])\n",
    "\n",
    "        new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.3)\n",
    "        for jk in range(new_boxes.shape[0]):\n",
    "            (x1, y1, x2, y2) = new_boxes[jk, :]\n",
    "            answer_anno = [key, int(x1+x2)/2, int(y1+y2)/2, x2-x1, y2-y1]\n",
    "            answer_annos.append(answer_anno)\n",
    "            # Calculate real coordinates on original image\n",
    "            if visualize:\n",
    "                (real_x1, real_y1, real_x2, real_y2) = (x1, y1, x2, y2)\n",
    "\n",
    "                cv2.rectangle(img, (real_x1, real_y1), (real_x2, real_y2), (\n",
    "                int(1), int(1), int(1)), 4)\n",
    "\n",
    "                textLabel = '{}: {}'.format(key, int(100 * new_probs[jk]))\n",
    "                all_dets.append((key, 100 * new_probs[jk]))\n",
    "\n",
    "                (retval, baseLine) = cv2.getTextSize(textLabel, cv2.FONT_HERSHEY_COMPLEX, 1, 1)\n",
    "                textOrg = (real_x1, real_y1 - 0)\n",
    "\n",
    "                cv2.rectangle(img, (textOrg[0] - 5, textOrg[1] + baseLine - 5),\n",
    "                              (textOrg[0] + retval[0] + 5, textOrg[1] - retval[1] - 5), (0, 0, 0), 1)\n",
    "                cv2.rectangle(img, (textOrg[0] - 5, textOrg[1] + baseLine - 5),\n",
    "                              (textOrg[0] + retval[0] + 5, textOrg[1] - retval[1] - 5), (255, 255, 255), -1)\n",
    "                cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "\n",
    "                print(all_dets)\n",
    "                plt.figure(figsize=(40, 30))\n",
    "                plt.grid()\n",
    "                plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "                plt.show()\n",
    "    answers.append(answer_annos)\n",
    "np.save(\"./data/test_answer_output\", np.array(answers))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%import tensorflow as tf\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}